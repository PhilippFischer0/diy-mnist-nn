{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comfy Kettenregel (Autograd DIY) - univariate, skalare Funktionen\n",
    "\n",
    "$$F(x) = f_1 \\circ f_2 = f_1(f_2(x)) \\Rightarrow f_1'(f_2(x)) \\cdot f'_2(x)$$\n",
    "\n",
    "$$F(x) = f_1 \\circ f_2 \\circ f_3 = f_1(f_2(f_3(x))) \\Rightarrow f_1'(f_2(f_3(x))) \\cdot f_2'(f_3\n",
    "(x)) \\cdot f_3'(x)$$\n",
    "\n",
    "## Aufgabe\n",
    "\n",
    "Ziel: Gradientenbasierte Optimierung von $f(x) = \\sqrt{\\frac{1}{e^{\\sin(x)}}}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.0 Operationen definieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_div_x(x: float, inner_derivative: float = 1) -> tuple[float, float]:\n",
    "\n",
    "    value = 1 / x\n",
    "    derivative = -inner_derivative / x**2\n",
    "\n",
    "    return value, derivative\n",
    "\n",
    "\n",
    "def sin(x: float, inner_derivative: float = 1) -> tuple[float, float]:\n",
    "\n",
    "    value = math.sin(x)\n",
    "    derivative = math.cos(x) * inner_derivative\n",
    "\n",
    "    return value, derivative\n",
    "\n",
    "\n",
    "def sqrt(x: float, inner_derivative: float = 1) -> tuple[float, float]:\n",
    "\n",
    "    value = math.sqrt(x)\n",
    "    derivative = 1 / (2 * math.sqrt(x)) * inner_derivative\n",
    "\n",
    "    return value, derivative\n",
    "\n",
    "\n",
    "def exp(x: float, inner_derivative: float = 1) -> tuple[float, float]:\n",
    "\n",
    "    value = math.exp(x)\n",
    "    derivative = math.exp(x) * inner_derivative\n",
    "\n",
    "    return value, derivative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Funktionsdefinition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_x(x: float) -> tuple[float, float]:\n",
    "\n",
    "    return sqrt(*one_div_x(*exp(*sin(x))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_start = 4.0  # starting value\n",
    "x_min = x_start - 8.0  # x-axis limits\n",
    "x_max = x_start + 8.0\n",
    "xs = []  # values for the animation\n",
    "ys = []\n",
    "\n",
    "lr = 1e-2  # step size\n",
    "significant_gradient = 1e-3  # termination criteria\n",
    "iter = 1  # counter\n",
    "\n",
    "while True:\n",
    "    y_measured, deriv = f_x(x_start)\n",
    "    if np.fabs(deriv) >= significant_gradient:\n",
    "        xs.append(x_start)\n",
    "        ys.append(y_measured)\n",
    "        x_start -= lr * deriv\n",
    "        print(iter, x_start, y_measured) if iter % 100 == 0 or iter == 1 else None\n",
    "    else:\n",
    "        xs.append(x_start)\n",
    "        ys.append(y_measured)\n",
    "        break\n",
    "    iter += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.0 Funktionsplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(x_min, x_max, 0.01)\n",
    "\n",
    "res = [f_x(_) for _ in x]\n",
    "y_measured, derivative = zip(*res)\n",
    "\n",
    "df = pd.DataFrame(\n",
    "    {\n",
    "        \"x\": x,\n",
    "        \"y\": y_measured,\n",
    "        \"derivative\": derivative,\n",
    "    }\n",
    ")\n",
    "\n",
    "px.line(df, x=\"x\", y=\"y\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Animation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the values\n",
    "x = np.arange(x_min, x_max, 0.01)\n",
    "\n",
    "res = [f_x(_) for _ in x]\n",
    "y_measured, _ = zip(*res)\n",
    "\n",
    "# define both graphs\n",
    "fig = go.Figure(\n",
    "    data=[\n",
    "        go.Scatter(\n",
    "            x=x,\n",
    "            y=y_measured,\n",
    "            mode=\"lines\",\n",
    "            line=dict(color=\"green\", width=1),\n",
    "            name=\"Function Graph\",\n",
    "        ),\n",
    "        go.Scatter(\n",
    "            x=[xs[0]],\n",
    "            y=[ys[0]],\n",
    "            mode=\"markers\",\n",
    "            marker=dict(color=\"red\", size=10),\n",
    "            name=\"Current Position\",\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# update layout parameters and add start button for animation\n",
    "fig.update_layout(\n",
    "    width=1400,\n",
    "    height=900,\n",
    "    xaxis=dict(range=(x_min, x_max), autorange=False),\n",
    "    yaxis=dict(\n",
    "        range=(np.min(y_measured) - 0.5, np.max(y_measured) + 0.5), autorange=False\n",
    "    ),\n",
    "    title_text=\"Gradient Descent Animation\",\n",
    "    # start button config\n",
    "    updatemenus=[\n",
    "        dict(\n",
    "            type=\"buttons\",\n",
    "            buttons=[\n",
    "                dict(\n",
    "                    args=[\n",
    "                        None,\n",
    "                        {\n",
    "                            \"frame\": {\"duration\": 5, \"redraw\": False},\n",
    "                            \"fromcurrent\": True,\n",
    "                            \"transition\": {\"duration\": 0, \"easing\": \"linear\"},\n",
    "                        },\n",
    "                    ],\n",
    "                    label=\"start\",\n",
    "                    method=\"animate\",\n",
    "                )\n",
    "            ],\n",
    "        )\n",
    "    ],\n",
    ")\n",
    "\n",
    "# specify the animation frames\n",
    "fig.update(\n",
    "    frames=[\n",
    "        go.Frame(data=[go.Scatter(x=[xs[k]], y=[ys[k]])], traces=[1])\n",
    "        for k in range(len(ys))\n",
    "    ]\n",
    ")\n",
    "\n",
    "# show result\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2024-11-18 \n",
    "\n",
    "Bisherige Ansatz hat folgende Limitierungen\n",
    "- funktioniert nur f체r Ausdr체cke in geschlossener Form, keine Kontrollflusslogik\n",
    "- inkompatibel mit bin채ren Operatoren (+, *, ...)\n",
    "- funktioniert nur in 1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), os.pardir)))\n",
    "\n",
    "from src import *\n",
    "from src.value import *\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "from plotly import graph_objects as go"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lineare Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0xDEADBEEF)\n",
    "\n",
    "x = np.linspace(-10, 10, 200)\n",
    "y_ideal = 2 * x - 2\n",
    "y_measured = y_ideal + np.random.randn(len(x)) * 1.5\n",
    "\n",
    "fig = px.scatter(x=x, y=y_measured)\n",
    "fig.add_trace(go.Scatter(x=x, y=y_ideal, mode=\"lines\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lineare Regression f(x) = m*x + c\n",
    "np.random.seed(0xDEADBEEF)\n",
    "x = np.linspace(-10, 10, 200)\n",
    "y_ideal = 2 * x - 2\n",
    "y_measured = y_ideal + np.random.randn(len(x)) * 1.5\n",
    "\n",
    "# Random init von m und c\n",
    "m = Value(np.random.random(size=None) * 5, name=\"slope\")\n",
    "c = Value(np.random.random(size=None) * 5, name=\"intercept\")\n",
    "\n",
    "\n",
    "# Lossfunktion definieren\n",
    "def loss(m: Value, c: Value) -> Value:\n",
    "    sum_error = Value(0.0)\n",
    "    for ii, x_i in enumerate(x):\n",
    "        sample_error = (m * x_i + c - y_measured[ii]) ** 2\n",
    "        sum_error = sum_error + sample_error\n",
    "    sum_error = sum_error / len(x)\n",
    "    return sum_error\n",
    "\n",
    "\n",
    "# Vergleich Algorithmus mit Arithmetik\n",
    "def partial_derivs(m, x, c):\n",
    "    sum_dloss_dm = 0.0\n",
    "    sum_dloss_dc = 0.0\n",
    "    for ii, x_i in enumerate(x):\n",
    "        # dloss_dm = 2 * (m * x_i + x_i * (c - y_measured[ii]))\n",
    "        dloss_dm = 2 * (m * x_i + c - y_measured[ii]) * x_i\n",
    "        dloss_dc = 2 * (m * x_i + c - y_measured[ii])\n",
    "        sum_dloss_dm += dloss_dm\n",
    "        sum_dloss_dc += dloss_dc\n",
    "\n",
    "    return sum_dloss_dm, sum_dloss_dc\n",
    "\n",
    "\n",
    "# Hyperparameter\n",
    "epochs = 1000\n",
    "lr = 1e-4\n",
    "ms = []\n",
    "cs = []\n",
    "m_grad = []\n",
    "c_grad = []\n",
    "\n",
    "# Trainingloop\n",
    "for i in range(epochs):\n",
    "\n",
    "    precision_loss = loss(m, c)\n",
    "\n",
    "    m.grad = 0\n",
    "    c.grad = 0\n",
    "    precision_loss.backward()\n",
    "\n",
    "    # - Zwischenergebnisse von (m und c) speichern\n",
    "    if i < 50 or i % 50 == 0:\n",
    "        ms.append(m.value)\n",
    "        cs.append(c.value)\n",
    "        m_grad.append(m.grad)\n",
    "        c_grad.append(c.grad)\n",
    "\n",
    "    # values anhand des negativen Gradienten akkumulieren\n",
    "    m.value -= lr * m.grad\n",
    "    c.value -= lr * c.grad\n",
    "\n",
    "\n",
    "print(f\"final m: {m.value}, final c: {c.value}, final loss: {precision_loss.value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vergleich analytisches Verfahren & Backwards Pass\n",
    "d = partial_derivs(4.034023390966637, x, 0.9569717983633408)\n",
    "\n",
    "print(d[0], m_grad[0], d[1], c_grad[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Animation der Regression und Plot der Loss Werte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ms = np.array(ms)\n",
    "cs = np.array(cs)\n",
    "\n",
    "data = []\n",
    "\n",
    "for i, (m, c) in enumerate(zip(ms, cs)):\n",
    "    ys = m * x + c\n",
    "    for xi, yi in zip(x, ys):\n",
    "        data.append(\n",
    "            {\n",
    "                \"x\": xi,\n",
    "                \"y\": yi,\n",
    "                \"frame\": i,\n",
    "                \"m\": m,\n",
    "                \"c\": c,\n",
    "            }\n",
    "        )\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "\n",
    "fig = px.line(df, x=\"x\", y=\"y\", animation_frame=\"frame\")\n",
    "fig.add_trace(go.Scatter(x=x, y=y_measured, mode=\"markers\"))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.line(x=range(len(m_grad)), y=np.abs(m_grad), log_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.line(x=range(len(c_grad)), y=np.abs(c_grad), log_y=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quadratische Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0xDEADBEED)\n",
    "\n",
    "x = np.linspace(-10, 10, 200)\n",
    "y_quad_ideal = 2.0 * x**2 - 1.5 * x - 4.0\n",
    "y_quad_measured = y_quad_ideal + np.random.randn(len(x)) * 20\n",
    "\n",
    "fig = px.scatter(x=x, y=y_quad_measured)\n",
    "fig.add_trace(go.Scatter(x=x, y=y_quad_ideal, mode=\"lines\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0xDEADBEED)\n",
    "\n",
    "a = Value(np.random.random(size=None) * 5, name=\"a\")\n",
    "b = Value(np.random.random(size=None) * 5, name=\"b\")\n",
    "c = Value(np.random.random(size=None) * 5, name=\"c\")\n",
    "\n",
    "x_quad = np.linspace(-10, 10, 200)\n",
    "y_quad_ideal = 2.0 * x_quad**2 - 1.5 * x_quad - 4.0\n",
    "y_quad_measured = y_quad_ideal + np.random.randn(len(x_quad)) * 20\n",
    "\n",
    "\n",
    "# Loss Funktion\n",
    "def loss_quad(x: np.ndarray, y: np.ndarray, a: Value, b: Value, c: Value) -> Value:\n",
    "    sum_loss = Value(0.0)\n",
    "    for x_i, y_i in zip(x, y):\n",
    "        sample_loss = (a * x_i**2 + b * x_i + c - y_i) ** 2\n",
    "        sum_loss = sum_loss + sample_loss\n",
    "    sum_loss = sum_loss / len(x)\n",
    "    sum_loss.name = \"loss\"\n",
    "    return sum_loss\n",
    "\n",
    "\n",
    "# liste/named tuple der zu optimierenden parameter\n",
    "def loss_quad(x: np.ndarray, y: np.ndarray, params: list) -> Value:\n",
    "    sum_loss = Value(0.0)\n",
    "    for x_i, y_i in zip(x, y):\n",
    "        sample_loss = (params[0] * x_i**2 + params[1] * x_i + params[2] - y_i) ** 2\n",
    "        sum_loss = sum_loss + sample_loss\n",
    "    sum_loss = sum_loss / len(x)\n",
    "    sum_loss.name = \"loss\"\n",
    "    return sum_loss\n",
    "\n",
    "\n",
    "def partials(\n",
    "    x: np.ndarray, y: np.ndarray, a: Value, b: Value, c: Value\n",
    ") -> tuple[float, float, float]:\n",
    "    dloss_da = 0.0\n",
    "    dloss_db = 0.0\n",
    "    dloss_dc = 0.0\n",
    "    for ii, x_i in enumerate(x):\n",
    "        dloss_da += 2 * (a * x_i**2 + b * x_i + c - y[ii]) * x_i**2\n",
    "        dloss_db += 2 * (a * x_i**2 + b * x_i + c - y[ii]) * x_i\n",
    "        dloss_dc += 2 * (a * x_i**2 + b * x_i + c - y[ii])\n",
    "\n",
    "    dloss_da = dloss_da / len(x)\n",
    "    dloss_db = dloss_db / len(x)\n",
    "    dloss_dc = dloss_dc / len(x)\n",
    "\n",
    "    return dloss_da, dloss_db, dloss_dc\n",
    "\n",
    "\n",
    "# Learning Rate eingrenzen -> Wann e+400 Gradienten\n",
    "# Ab lr von 5e-4 funktioniert Gradient descent nicht mehr\n",
    "# Hyperparameter\n",
    "epochs = 5000\n",
    "lr = 4e-4\n",
    "\n",
    "# Plot parameter\n",
    "a_vals = []\n",
    "a_grad = []\n",
    "b_vals = []\n",
    "b_grad = []\n",
    "c_vals = []\n",
    "c_grad = []\n",
    "losses = []\n",
    "\n",
    "params = [a, b, c]\n",
    "# Trainingsloop\n",
    "for i in range(epochs):\n",
    "    loss = loss_quad(x=x_quad, y=y_quad_measured, params=params)\n",
    "\n",
    "    for p in params:\n",
    "        p.grad = 0.0\n",
    "\n",
    "    loss.backward()\n",
    "    if i < 50 or i % 50 == 0:\n",
    "        a_vals.append(a.value)\n",
    "        a_grad.append(a.grad)\n",
    "        b_vals.append(b.value)\n",
    "        b_grad.append(b.grad)\n",
    "        c_vals.append(c.value)\n",
    "        c_grad.append(c.grad)\n",
    "        losses.append(loss.value)\n",
    "    # live debugging statement\n",
    "    if i % 100 == 0:\n",
    "        print(f\"{i}: loss: {loss}, a: {a}, b: {b}, c: {c}\")\n",
    "\n",
    "    for p in params:\n",
    "        p.value -= lr * p.grad\n",
    "\n",
    "print(f\"Final loss: {loss}, final a: {a}, final b: {b}, final c: {c}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# idealer loss Wert\n",
    "approximate_loss = loss_quad(\n",
    "    x_quad, y_quad_measured, [Value(2.0), Value(-1.5), Value(-4.0)]\n",
    ")\n",
    "approximate_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "part = partials(x, y_quad_measured, a_vals[0], b_vals[0], c_vals[0])\n",
    "\n",
    "print(part[0], a_grad[0], part[1], b_grad[0], part[2], c_grad[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Animation der quadratischen Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_vals = np.array(a_vals)\n",
    "b_vals = np.array(b_vals)\n",
    "c_vals = np.array(c_vals)\n",
    "\n",
    "data = []\n",
    "\n",
    "for i, (a, b, c) in enumerate(zip(a_vals, b_vals, c_vals)):\n",
    "    ys = a * x**2 + b * x + c\n",
    "    for xi, yi in zip(x_quad, ys):\n",
    "        data.append(\n",
    "            {\n",
    "                \"x\": xi,\n",
    "                \"y\": yi,\n",
    "                \"frame\": i,\n",
    "            }\n",
    "        )\n",
    "\n",
    "df = pd.DataFrame(data=data)\n",
    "\n",
    "fig = px.line(df, x=\"x\", y=\"y\", animation_frame=\"frame\")\n",
    "fig.add_trace(go.Scatter(x=x, y=y_quad_measured, mode=\"markers\"))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot der Gradienten-Werte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Betr채ge der Gradienten plotten\n",
    "px.line(x=range(len(a_grad)), y=np.abs(a_grad), log_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.line(x=range(len(b_grad)), y=np.abs(b_grad), log_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.line(x=range(len(c_grad)), y=np.abs(c_grad), log_y=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot der Loss Werte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.line(x=range(len(losses)), y=losses)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diy-mnist-nn-ZXZBkNkJ-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
