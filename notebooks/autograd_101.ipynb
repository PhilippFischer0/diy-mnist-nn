{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comfy Kettenregel (Autograd DIY) - univariate, skalare Funktionen\n",
    "\n",
    "$$F(x) = f_1 \\circ f_2 = f_1(f_2(x)) \\Rightarrow f_1'(f_2(x)) \\cdot f'_2(x)$$\n",
    "\n",
    "$$F(x) = f_1 \\circ f_2 \\circ f_3 = f_1(f_2(f_3(x))) \\Rightarrow f_1'(f_2(f_3(x))) \\cdot f_2'(f_3\n",
    "(x)) \\cdot f_3'(x)$$\n",
    "\n",
    "## Aufgabe\n",
    "\n",
    "Ziel: Gradientenbasierte Optimierung von $f(x) = \\sqrt{\\frac{1}{e^{\\sin(x)}}}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.0 Operationen definieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_div_x(x: float, inner_derivative: float = 1) -> tuple[float, float]:\n",
    "\n",
    "    value = 1 / x\n",
    "    derivative = -inner_derivative / x**2\n",
    "\n",
    "    return value, derivative\n",
    "\n",
    "\n",
    "def sin(x: float, inner_derivative: float = 1) -> tuple[float, float]:\n",
    "\n",
    "    value = math.sin(x)\n",
    "    derivative = math.cos(x) * inner_derivative\n",
    "\n",
    "    return value, derivative\n",
    "\n",
    "\n",
    "def sqrt(x: float, inner_derivative: float = 1) -> tuple[float, float]:\n",
    "\n",
    "    value = math.sqrt(x)\n",
    "    derivative = 1 / (2 * math.sqrt(x)) * inner_derivative\n",
    "\n",
    "    return value, derivative\n",
    "\n",
    "\n",
    "def exp(x: float, inner_derivative: float = 1) -> tuple[float, float]:\n",
    "\n",
    "    value = math.exp(x)\n",
    "    derivative = math.exp(x) * inner_derivative\n",
    "\n",
    "    return value, derivative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Funktionsdefinition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_x(x: float) -> tuple[float, float]:\n",
    "\n",
    "    return sqrt(*one_div_x(*exp(*sin(x))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_start = 4.0  # starting value\n",
    "x_min = x_start - 8.0  # x-axis limits\n",
    "x_max = x_start + 8.0\n",
    "xs = []  # values for the animation\n",
    "ys = []\n",
    "\n",
    "lr = 1e-2  # step size\n",
    "significant_gradient = 1e-3  # termination criteria\n",
    "iter = 1  # counter\n",
    "\n",
    "while True:\n",
    "    y_measured, deriv = f_x(x_start)\n",
    "    if np.fabs(deriv) >= significant_gradient:\n",
    "        xs.append(x_start)\n",
    "        ys.append(y_measured)\n",
    "        x_start -= lr * deriv\n",
    "        print(iter, x_start, y_measured) if iter % 100 == 0 or iter == 1 else None\n",
    "    else:\n",
    "        xs.append(x_start)\n",
    "        ys.append(y_measured)\n",
    "        break\n",
    "    iter += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.0 Funktionsplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(x_min, x_max, 0.01)\n",
    "\n",
    "res = [f_x(_) for _ in x]\n",
    "y_measured, derivative = zip(*res)\n",
    "\n",
    "df = pd.DataFrame(\n",
    "    {\n",
    "        \"x\": x,\n",
    "        \"y\": y_measured,\n",
    "        \"derivative\": derivative,\n",
    "    }\n",
    ")\n",
    "\n",
    "px.line(df, x=\"x\", y=\"y\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Animation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the values\n",
    "x = np.arange(x_min, x_max, 0.01)\n",
    "\n",
    "res = [f_x(_) for _ in x]\n",
    "y_measured, _ = zip(*res)\n",
    "\n",
    "# define both graphs\n",
    "fig = go.Figure(\n",
    "    data=[\n",
    "        go.Scatter(\n",
    "            x=x,\n",
    "            y=y_measured,\n",
    "            mode=\"lines\",\n",
    "            line=dict(color=\"green\", width=1),\n",
    "            name=\"Function Graph\",\n",
    "        ),\n",
    "        go.Scatter(\n",
    "            x=[xs[0]],\n",
    "            y=[ys[0]],\n",
    "            mode=\"markers\",\n",
    "            marker=dict(color=\"red\", size=10),\n",
    "            name=\"Current Position\",\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# update layout parameters and add start button for animation\n",
    "fig.update_layout(\n",
    "    width=1400,\n",
    "    height=900,\n",
    "    xaxis=dict(range=(x_min, x_max), autorange=False),\n",
    "    yaxis=dict(\n",
    "        range=(np.min(y_measured) - 0.5, np.max(y_measured) + 0.5), autorange=False\n",
    "    ),\n",
    "    title_text=\"Gradient Descent Animation\",\n",
    "    # start button config\n",
    "    updatemenus=[\n",
    "        dict(\n",
    "            type=\"buttons\",\n",
    "            buttons=[\n",
    "                dict(\n",
    "                    args=[\n",
    "                        None,\n",
    "                        {\n",
    "                            \"frame\": {\"duration\": 5, \"redraw\": False},\n",
    "                            \"fromcurrent\": True,\n",
    "                            \"transition\": {\"duration\": 0, \"easing\": \"linear\"},\n",
    "                        },\n",
    "                    ],\n",
    "                    label=\"start\",\n",
    "                    method=\"animate\",\n",
    "                )\n",
    "            ],\n",
    "        )\n",
    "    ],\n",
    ")\n",
    "\n",
    "# specify the animation frames\n",
    "fig.update(\n",
    "    frames=[\n",
    "        go.Frame(data=[go.Scatter(x=[xs[k]], y=[ys[k]])], traces=[1])\n",
    "        for k in range(len(ys))\n",
    "    ]\n",
    ")\n",
    "\n",
    "# show result\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2024-11-18 \n",
    "\n",
    "Bisherige Ansatz hat folgende Limitierungen\n",
    "- funktioniert nur für Ausdrücke in geschlossener Form, keine Kontrollflusslogik\n",
    "- inkompatibel mit binären Operatoren (+, *, ...)\n",
    "- funktioniert nur in 1D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Value Klasse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import graphviz\n",
    "from IPython.display import display\n",
    "import math\n",
    "\n",
    "\n",
    "class Value:\n",
    "    def __init__(\n",
    "        self, value: float, ancestors: tuple[Value, ...] = (), name=\"\", operand=\"\"\n",
    "    ):\n",
    "        self.value = value\n",
    "        self.ancestors = ancestors\n",
    "        self.name = name\n",
    "        self.grad = 0.0\n",
    "        self._backward = lambda: None\n",
    "        self.operand = operand\n",
    "\n",
    "    # make values printable\n",
    "    def __repr__(self) -> str:\n",
    "        return f\"{self.name}, value={self.value}, grad={self.grad}\"\n",
    "\n",
    "    # Addition\n",
    "    def __add__(self, other: Value) -> Value:\n",
    "        if not isinstance(other, Value):\n",
    "            other = Value(other)\n",
    "        result = Value(self.value + other.value, (self, other), name=\"add\", operand=\"+\")\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += result.grad\n",
    "            other.grad += result.grad\n",
    "\n",
    "        result._backward = _backward\n",
    "        return result\n",
    "\n",
    "    def __iadd__(self, other: Value) -> Value:\n",
    "        if not isinstance(other, Value):\n",
    "            other = Value(other)\n",
    "        result = Value(\n",
    "            self.value + other.value, (self, other), name=\"iadd\", operand=\"+=\"\n",
    "        )\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += result.grad\n",
    "            other.grad += result.grad\n",
    "\n",
    "        result._backward = _backward\n",
    "        return result\n",
    "\n",
    "    def __radd__(self, other: Value):\n",
    "        return self + other\n",
    "\n",
    "    # Subtraktion\n",
    "    def __sub__(self, other: Value) -> Value:\n",
    "        if not isinstance(other, Value):\n",
    "            other = Value(other)\n",
    "        result = Value(self.value - other.value, (self, other), name=\"sub\", operand=\"-\")\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += 1.0 * result.grad\n",
    "            other.grad += -1.0 * result.grad\n",
    "\n",
    "        result._backward = _backward\n",
    "        return result\n",
    "\n",
    "    def __rsub__(self, other: Value) -> Value:\n",
    "        return self - other\n",
    "\n",
    "    # Multiplikation\n",
    "    def __mul__(self, other: Value) -> Value:\n",
    "        if not isinstance(other, Value):\n",
    "            other = Value(other)\n",
    "        result = Value(self.value * other.value, (self, other), name=\"mul\", operand=\"*\")\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += other.value * result.grad\n",
    "            other.grad += self.value * result.grad\n",
    "\n",
    "        result._backward = _backward\n",
    "        return result\n",
    "\n",
    "    def __rmul__(self, other: Value) -> Value:\n",
    "        return self * other\n",
    "\n",
    "    # Floatingpointdivision\n",
    "    def __truediv__(self, other: Value) -> Value:\n",
    "        if not isinstance(other, Value):\n",
    "            other = Value(other)\n",
    "        result = Value(self.value / other.value, (self, other), name=\"div\", operand=\"/\")\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += 1 / other.value * result.grad\n",
    "            other.grad += -self.value / other.value**2 * result.grad\n",
    "\n",
    "        result._backward = _backward\n",
    "        return result\n",
    "\n",
    "    def __rtruediv__(self, other: Value) -> Value:\n",
    "        return self / other\n",
    "\n",
    "    # Potenzierung (x**n)\n",
    "    def __pow__(self, other: Value) -> Value:\n",
    "        if not isinstance(other, Value):\n",
    "            other = Value(other)\n",
    "        result = Value(self.value**other.value, (self, other), name=\"pow\", operand=\"^\")\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += other.value * self.value ** (other.value - 1.0) * result.grad\n",
    "            # assert self.value >= 0, \"cannot compute log with negative base\n",
    "            other.grad += self.value**other.value * np.log(self.value) * result.grad\n",
    "            # print(self.grad, other.grad)\n",
    "\n",
    "        result._backward = _backward\n",
    "        return result\n",
    "\n",
    "    # Exponentierung (e**x)\n",
    "    def exp(self) -> Value:\n",
    "        result = Value(np.exp(self.value), (self,), name=\"exp\", operand=\"e^\")\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += result.value * result.grad\n",
    "\n",
    "        result._backward = _backward\n",
    "        return result\n",
    "\n",
    "    def log(self) -> Value:\n",
    "        result = Value(np.log(self.value), (self,), name=\"log\")\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += 1 / self.value * result.grad\n",
    "\n",
    "        result._backward = _backward\n",
    "        return result\n",
    "\n",
    "    # backwards up until this point\n",
    "    # Negation\n",
    "    def __neg__(self) -> Value:\n",
    "        result = Value(-self.value, (self,), name=\"neg\", operand=\"-\")\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += -result.grad\n",
    "\n",
    "        result._backward = _backward\n",
    "        return result\n",
    "\n",
    "    def sigmoid(self) -> Value:\n",
    "        sigmoid_value = 1 / (1 + np.exp(-self.value))\n",
    "        result = Value(sigmoid_value, (self,), name=\"sigmoid\")\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += sigmoid_value * (1 - sigmoid_value) * result.grad\n",
    "\n",
    "        result._backward = _backward\n",
    "        return result\n",
    "\n",
    "    # make this make sense\n",
    "    # def softmax(self) -> Value:\n",
    "    #     exp_element = Value(np.exp(self.value - np.max(self.value)))\n",
    "    #     result = Value(exp_element / np.sum(exp_element, keepdims=True))  # -> returns 1\n",
    "\n",
    "    #     # finish this\n",
    "    #     def _backward():\n",
    "    #         self.grad += (\n",
    "    #             (exp_element * np.sum(exp_element) - np.sum(exp_element) * exp_element)\n",
    "    #             / (np.sum(exp_element)) ** 2\n",
    "    #             * result.grad\n",
    "    #         )  # -> returns 0\n",
    "\n",
    "    #     result._backward = _backward\n",
    "    #     return result\n",
    "\n",
    "    # Vergleichsoperatoren <, >, >=, <=\n",
    "    def __lt__(self, other: Value) -> bool:\n",
    "        if not isinstance(other, Value):\n",
    "            other = Value(other)\n",
    "        return self.value < other.value\n",
    "\n",
    "    def __gt__(self, other: Value) -> bool:\n",
    "        if not isinstance(other, Value):\n",
    "            other = Value(other)\n",
    "        return self.value > other.value\n",
    "\n",
    "    def __le__(self, other: Value) -> bool:\n",
    "        if not isinstance(other, Value):\n",
    "            other = Value(other)\n",
    "        return self.value <= other.value\n",
    "\n",
    "    def __ge__(self, other: Value) -> bool:\n",
    "        if not isinstance(other, Value):\n",
    "            other = Value(other)\n",
    "        return self.value >= other.value\n",
    "\n",
    "    def backward(self) -> None:\n",
    "        # iterate through the graph, calculate gradients and update nodes\n",
    "        topo_sorted_nodes = []\n",
    "        visited = set()\n",
    "\n",
    "        # topological sort of the nodes\n",
    "        def build_topo(node: Value):\n",
    "            if node not in visited:\n",
    "                visited.add(node)\n",
    "                for ancestor in node.ancestors:\n",
    "                    build_topo(ancestor)\n",
    "                topo_sorted_nodes.append(node)\n",
    "\n",
    "        build_topo(self)\n",
    "\n",
    "        self.grad = 1.0\n",
    "        for node in reversed(topo_sorted_nodes):\n",
    "            node._backward()\n",
    "\n",
    "    def plot_graph(self):\n",
    "        # \"graph visualization python\", graphviz\n",
    "        dot = graphviz.Digraph(format=\"svg\", graph_attr={\"rankdir\": \"LR\"})\n",
    "\n",
    "        def add_nodes(dot: graphviz.Digraph, node: Value):\n",
    "            label = f\"{node.name}|value={node.value}|grad={node.grad}\"\n",
    "            unique_node_name = str(id(node))\n",
    "\n",
    "            # add value nodes to graph\n",
    "            dot.node(\n",
    "                name=unique_node_name,\n",
    "                label=label,\n",
    "                shape=\"record\",\n",
    "                color=(\n",
    "                    \"lightgreen\" if node.ancestors == () and node.name != \"\" else None\n",
    "                ),  # check if input\n",
    "                style=\"filled\",\n",
    "            )\n",
    "\n",
    "            if node.operand:  # check if there is an operand to display\n",
    "                op_name = unique_node_name + node.operand\n",
    "                # add operation node\n",
    "                dot.node(\n",
    "                    name=op_name,\n",
    "                    label=node.operand,\n",
    "                )\n",
    "                # draw edge from operand to result\n",
    "                dot.edge(op_name, unique_node_name)\n",
    "\n",
    "            # iterate through the ancestors to build the whole graph\n",
    "            for ancestor in node.ancestors:\n",
    "                ancestor_name = add_nodes(dot, ancestor)\n",
    "                if node.operand:\n",
    "                    # ensure ancestor edge goes to operand node if it exists\n",
    "                    dot.edge(ancestor_name, op_name)\n",
    "                else:\n",
    "                    dot.edge(ancestor_name, unique_node_name)\n",
    "\n",
    "            return unique_node_name\n",
    "\n",
    "        add_nodes(dot, self)\n",
    "        display(dot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize values\n",
    "x = Value(5.0, name=\"x\")\n",
    "y_measured = Value(2.5, name=\"y\")\n",
    "\n",
    "a = Value(2.5, name=\"a\")\n",
    "b = Value(3.0, name=\"b\")\n",
    "c = Value(1.5, name=\"c\")\n",
    "\n",
    "# Folgendes sollte ausfühbar sein:\n",
    "print(x + y_measured)\n",
    "print(x * y_measured)\n",
    "print(x - y_measured)\n",
    "print(x / y_measured)\n",
    "print(x**y_measured)\n",
    "print(x**5)\n",
    "print(-x)\n",
    "print(x == y_measured)\n",
    "\n",
    "\n",
    "def foo(a: Value, b: Value, c: Value):\n",
    "    if a > Value(2):\n",
    "        return a * b + c\n",
    "    return a - b * c\n",
    "\n",
    "\n",
    "def f(a: Value, b: Value, c: Value):\n",
    "    # (((b**2) * c) + a)\n",
    "    x = b**2 * c\n",
    "    y = a + x\n",
    "    return y\n",
    "\n",
    "\n",
    "z1 = foo(a, b, c)\n",
    "z1.plot_graph()\n",
    "\n",
    "z2 = foo(Value(-1, name=\"a2\"), b, c)\n",
    "z2.plot_graph()\n",
    "\n",
    "z3 = f(a, b, c)\n",
    "z3.plot_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lineare Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import plotly.express as px\n",
    "from plotly import graph_objects as go\n",
    "\n",
    "np.random.seed(0xDEADBEEF)\n",
    "\n",
    "x = np.linspace(-10, 10, 200)\n",
    "y_ideal = 2 * x - 2\n",
    "y_measured = y_ideal + np.random.randn(len(x)) * 1.5\n",
    "\n",
    "fig = px.scatter(x=x, y=y_measured)\n",
    "fig.add_trace(go.Scatter(x=x, y=y_ideal, mode=\"lines\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lineare Regression f(x) = m*x + c\n",
    "np.random.seed(0xDEADBEEF)\n",
    "x = np.linspace(-10, 10, 200)\n",
    "y_ideal = 2 * x - 2\n",
    "y_measured = y_ideal + np.random.randn(len(x)) * 1.5\n",
    "\n",
    "# Random init von m und c\n",
    "m = Value(np.random.random(size=None) * 5, name=\"slope\")\n",
    "c = Value(np.random.random(size=None) * 5, name=\"intercept\")\n",
    "\n",
    "\n",
    "# Lossfunktion definieren\n",
    "def loss(m: Value, c: Value) -> Value:\n",
    "    sum_error = Value(0.0)\n",
    "    for ii, x_i in enumerate(x):\n",
    "        sample_error = (m * x_i + c - y_measured[ii]) ** 2\n",
    "        sum_error = sum_error + sample_error\n",
    "    sum_error = sum_error / len(x)\n",
    "    return sum_error\n",
    "\n",
    "\n",
    "# Vergleich Algorithmus mit Arithmetik\n",
    "def partial_derivs(m, x, c):\n",
    "    sum_dloss_dm = 0.0\n",
    "    sum_dloss_dc = 0.0\n",
    "    for ii, x_i in enumerate(x):\n",
    "        # dloss_dm = 2 * (m * x_i + x_i * (c - y_measured[ii]))\n",
    "        dloss_dm = 2 * (m * x_i + c - y_measured[ii]) * x_i\n",
    "        dloss_dc = 2 * (m * x_i + c - y_measured[ii])\n",
    "        sum_dloss_dm += dloss_dm\n",
    "        sum_dloss_dc += dloss_dc\n",
    "\n",
    "    return sum_dloss_dm, sum_dloss_dc\n",
    "\n",
    "\n",
    "# Hyperparameter\n",
    "epochs = 1000\n",
    "lr = 1e-4\n",
    "ms = []\n",
    "cs = []\n",
    "m_grad = []\n",
    "c_grad = []\n",
    "\n",
    "# Trainingloop\n",
    "for i in range(epochs):\n",
    "\n",
    "    precision_loss = loss(m, c)\n",
    "\n",
    "    m.grad = 0\n",
    "    c.grad = 0\n",
    "    precision_loss.backward()\n",
    "\n",
    "    # - Zwischenergebnisse von (m und c) speichern\n",
    "    if i < 50 or i % 50 == 0:\n",
    "        ms.append(m.value)\n",
    "        cs.append(c.value)\n",
    "        m_grad.append(m.grad)\n",
    "        c_grad.append(c.grad)\n",
    "\n",
    "    # values anhand des negativen Gradienten akkumulieren\n",
    "    m.value -= lr * m.grad\n",
    "    c.value -= lr * c.grad\n",
    "\n",
    "\n",
    "print(f\"final m: {m.value}, final c: {c.value}, final loss: {precision_loss.value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vergleich analytisches Verfahren & Backwards Pass\n",
    "d = partial_derivs(4.034023390966637, x, 0.9569717983633408)\n",
    "\n",
    "print(d[0], m_grad[0], d[1], c_grad[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Animation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "ms = np.array(ms)\n",
    "cs = np.array(cs)\n",
    "\n",
    "data = []\n",
    "\n",
    "for i, (m, c) in enumerate(zip(ms, cs)):\n",
    "    ys = m * x + c\n",
    "    for xi, yi in zip(x, ys):\n",
    "        data.append(\n",
    "            {\n",
    "                \"x\": xi,\n",
    "                \"y\": yi,\n",
    "                \"frame\": i,\n",
    "                \"m\": m,\n",
    "                \"c\": c,\n",
    "            }\n",
    "        )\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "\n",
    "fig = px.line(df, x=\"x\", y=\"y\", animation_frame=\"frame\")\n",
    "fig.add_trace(go.Scatter(x=x, y=y_measured, mode=\"markers\"))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.line(x=range(len(m_grad)), y=np.abs(m_grad), log_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.line(x=range(len(c_grad)), y=np.abs(c_grad), log_y=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lossfunktion definieren\n",
    "def loss(m: Value, c: Value, x, y) -> Value:\n",
    "    sum_error = Value(0.0)\n",
    "    for ii, x_i in enumerate(x):\n",
    "        sample_error = (m * x_i + c - y[ii]) ** 2\n",
    "        sum_error = sum_error + sample_error\n",
    "    return sum_error\n",
    "\n",
    "\n",
    "# Vergleich Algorithmus mit Arithmetik\n",
    "def partial_derivs(m, c, x, y):\n",
    "    sum_dloss_dm = 0.0\n",
    "    sum_dloss_dc = 0.0\n",
    "    for ii, x_i in enumerate(x):\n",
    "        dloss_dm = 2 * (m * x_i + c - y[ii]) * x_i\n",
    "        dloss_dc = 2 * (m * x_i + c - y[ii])\n",
    "        sum_dloss_dm += dloss_dm\n",
    "        sum_dloss_dc += dloss_dc\n",
    "\n",
    "    return sum_dloss_dm, sum_dloss_dc\n",
    "\n",
    "\n",
    "np.random.seed(0xDEADBEEF)\n",
    "x = np.linspace(-10, 10, 10)\n",
    "y_ideal = 2 * x - 2\n",
    "y_measured = y_ideal + np.random.randn(len(x)) * 1.5\n",
    "\n",
    "# Random init von m und c\n",
    "m = Value(np.random.random(size=None) * 5, name=\"slope\")\n",
    "c = Value(np.random.random(size=None) * 5, name=\"intercept\")\n",
    "\n",
    "\n",
    "L = loss(m, c, x, y_measured)\n",
    "\n",
    "L.backward()\n",
    "\n",
    "ds = partial_derivs(m.value, c.value, x, y_measured)\n",
    "\n",
    "print(m.grad, c.grad, ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L.plot_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quadratische Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0xDEADBEED)\n",
    "\n",
    "x = np.linspace(-10, 10, 200)\n",
    "y_quad_ideal = 2.0 * x**2 - 1.5 * x - 4.0\n",
    "y_quad_measured = y_quad_ideal + np.random.randn(len(x)) * 20\n",
    "\n",
    "fig = px.scatter(x=x, y=y_quad_measured)\n",
    "fig.add_trace(go.Scatter(x=x, y=y_quad_ideal, mode=\"lines\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0xDEADBEED)\n",
    "\n",
    "a = Value(np.random.random(size=None) * 5, name=\"a\")\n",
    "b = Value(np.random.random(size=None) * 5, name=\"b\")\n",
    "c = Value(np.random.random(size=None) * 5, name=\"c\")\n",
    "\n",
    "x_quad = np.linspace(-10, 10, 200)\n",
    "y_quad_ideal = 2.0 * x_quad**2 - 1.5 * x_quad - 4.0\n",
    "y_quad_measured = y_quad_ideal + np.random.randn(len(x_quad)) * 20\n",
    "\n",
    "\n",
    "# Loss Funktion\n",
    "def loss_quad(x: np.ndarray, y: np.ndarray, a: Value, b: Value, c: Value) -> Value:\n",
    "    sum_loss = Value(0.0)\n",
    "    for x_i, y_i in zip(x, y):\n",
    "        sample_loss = (a * x_i**2 + b * x_i + c - y_i) ** 2\n",
    "        sum_loss = sum_loss + sample_loss\n",
    "    sum_loss = sum_loss / len(x)\n",
    "    sum_loss.name = \"loss\"\n",
    "    return sum_loss\n",
    "\n",
    "\n",
    "# liste/named tuple der zu optimierenden parameter\n",
    "def loss_quad(x: np.ndarray, y: np.ndarray, params: list) -> Value:\n",
    "    sum_loss = Value(0.0)\n",
    "    for x_i, y_i in zip(x, y):\n",
    "        sample_loss = (params[0] * x_i**2 + params[1] * x_i + params[2] - y_i) ** 2\n",
    "        sum_loss = sum_loss + sample_loss\n",
    "    sum_loss = sum_loss / len(x)\n",
    "    sum_loss.name = \"loss\"\n",
    "    return sum_loss\n",
    "\n",
    "\n",
    "def partials(\n",
    "    x: np.ndarray, y: np.ndarray, a: Value, b: Value, c: Value\n",
    ") -> tuple[float, float, float]:\n",
    "    dloss_da = 0.0\n",
    "    dloss_db = 0.0\n",
    "    dloss_dc = 0.0\n",
    "    for ii, x_i in enumerate(x):\n",
    "        dloss_da += 2 * (a * x_i**2 + b * x_i + c - y[ii]) * x_i**2\n",
    "        dloss_db += 2 * (a * x_i**2 + b * x_i + c - y[ii]) * x_i\n",
    "        dloss_dc += 2 * (a * x_i**2 + b * x_i + c - y[ii])\n",
    "\n",
    "    dloss_da = dloss_da / len(x)\n",
    "    dloss_db = dloss_db / len(x)\n",
    "    dloss_dc = dloss_dc / len(x)\n",
    "\n",
    "    return dloss_da, dloss_db, dloss_dc\n",
    "\n",
    "\n",
    "# Learning Rate eingrenzen -> Wann e+400 Gradienten\n",
    "# Ab lr von 5e-4 funktioniert Gradient descent nicht mehr\n",
    "# Hyperparameter\n",
    "epochs = 5000\n",
    "lr = 4e-4\n",
    "\n",
    "# Plot values\n",
    "a_vals = []\n",
    "a_grad = []\n",
    "b_vals = []\n",
    "b_grad = []\n",
    "c_vals = []\n",
    "c_grad = []\n",
    "losses = []\n",
    "\n",
    "params = [a, b, c]\n",
    "# Trainingsloop\n",
    "for i in range(epochs):\n",
    "    loss = loss_quad(x=x_quad, y=y_quad_measured, params=params)\n",
    "\n",
    "    for p in params:\n",
    "        p.grad = 0.0\n",
    "\n",
    "    loss.backward()\n",
    "    if i < 50 or i % 50 == 0:\n",
    "        a_vals.append(a.value)\n",
    "        a_grad.append(a.grad)\n",
    "        b_vals.append(b.value)\n",
    "        b_grad.append(b.grad)\n",
    "        c_vals.append(c.value)\n",
    "        c_grad.append(c.grad)\n",
    "        losses.append(loss.value)\n",
    "    # live debugging statement\n",
    "    if i % 100 == 0:\n",
    "        print(f\"{i}: loss: {loss}, a: {a}, b: {b}, c: {c}\")\n",
    "\n",
    "    for p in params:\n",
    "        p.value -= lr * p.grad\n",
    "\n",
    "print(f\"Final loss: {loss}, final a: {a}, final b: {b}, final c: {c}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "approximate_loss = loss_quad(\n",
    "    x_quad, y_quad_measured, [Value(2.0), Value(-1.5), Value(-4.0)]\n",
    ")\n",
    "approximate_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "part = partials(x, y_quad_measured, a_vals[0], b_vals[0], c_vals[0])\n",
    "\n",
    "print(part[0], a_grad[0], part[1], b_grad[0], part[2], c_grad[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Animation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_vals = np.array(a_vals)\n",
    "b_vals = np.array(b_vals)\n",
    "c_vals = np.array(c_vals)\n",
    "\n",
    "data = []\n",
    "\n",
    "for i, (a, b, c) in enumerate(zip(a_vals, b_vals, c_vals)):\n",
    "    ys = a * x**2 + b * x + c\n",
    "    for xi, yi in zip(x_quad, ys):\n",
    "        data.append(\n",
    "            {\n",
    "                \"x\": xi,\n",
    "                \"y\": yi,\n",
    "                \"frame\": i,\n",
    "            }\n",
    "        )\n",
    "\n",
    "df = pd.DataFrame(data=data)\n",
    "\n",
    "fig = px.line(df, x=\"x\", y=\"y\", animation_frame=\"frame\")\n",
    "fig.add_trace(go.Scatter(x=x, y=y_quad_measured, mode=\"markers\"))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beträge der Gradienten plotten\n",
    "px.line(x=range(len(a_grad)), y=np.abs(a_grad), log_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.line(x=range(len(b_grad)), y=np.abs(b_grad), log_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.line(x=range(len(c_grad)), y=np.abs(c_grad), log_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.line(x=range(len(losses)), y=losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST binary classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: \n",
    "- Filtern 2 Zahlen -> MNIST Datensatz done\n",
    "- runterskalieren auf 8x8-10x10 pixel\n",
    "- binäre Klassifikation -> 64/100 -> 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def parse_mnist_data(\n",
    "    idx_file_training_samples: str,\n",
    "    idx_file_training_labels: str,\n",
    "    number_1: int,\n",
    "    number_2: int,\n",
    ") -> tuple[np.ndarray, np.ndarray]:\n",
    "\n",
    "    training_labels = parse_mnist_labels(idx_file_training_labels)\n",
    "    training_samples = parse_mnist_images(idx_file_training_samples)\n",
    "\n",
    "    # filter only two numbers with a mask\n",
    "    mask = (training_labels.flatten() == number_1) | (\n",
    "        training_labels.flatten() == number_2\n",
    "    )\n",
    "    filtered_labels = training_labels[mask]\n",
    "    filtered_samples = training_samples[mask]\n",
    "\n",
    "    # downscale images with pillow\n",
    "    downscaled_samples = np.array(\n",
    "        # Image.Resampling.LANCZOS für Antialiasing\n",
    "        [\n",
    "            Image.fromarray(img).resize((10, 10), Image.Resampling.LANCZOS)\n",
    "            for img in filtered_samples\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    downscaled_samples = downscaled_samples / 255\n",
    "\n",
    "    return downscaled_samples, filtered_labels\n",
    "\n",
    "\n",
    "def parse_mnist_images(idx_file_path: str) -> np.ndarray:\n",
    "    with open(idx_file_path, \"rb\") as f:\n",
    "\n",
    "        # read magic number\n",
    "        f.read(4)\n",
    "        num_img = int.from_bytes(f.read(4), \"big\")\n",
    "        num_rows = int.from_bytes(f.read(4), \"big\")\n",
    "        num_cols = int.from_bytes(f.read(4), \"big\")\n",
    "\n",
    "        data = f.read()\n",
    "        out = np.ndarray((num_img, num_rows, num_cols), np.uint8, data)\n",
    "        return out\n",
    "\n",
    "\n",
    "def parse_mnist_labels(idx_file_path: str) -> np.ndarray:\n",
    "    with open(idx_file_path, \"rb\") as f:\n",
    "\n",
    "        # read magic number\n",
    "        f.read(4)\n",
    "        num_item = int.from_bytes(f.read(4), \"big\")\n",
    "\n",
    "        data = f.read()\n",
    "        out = np.ndarray((num_item, 1), np.uint8, data)\n",
    "        return out\n",
    "\n",
    "\n",
    "def plot_image(img: np.ndarray) -> plt.Figure:\n",
    "    assert len(img.shape) == 2, \"input must be 2-dimensional (single image)\"\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.axis(\"off\")\n",
    "    ax.imshow(img * 255, cmap=\"gray\")\n",
    "\n",
    "    plt.close()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neuron:\n",
    "    def __init__(self, num_inputs: int) -> None:\n",
    "        self.weights = [Value(np.random.random(size=None)) for _ in range(num_inputs)]\n",
    "        self.bias = Value(1.0, name=\"bias\")\n",
    "\n",
    "    def __call__(self, x: np.ndarray) -> Value:\n",
    "        # implement f(x) = activation (bias + sum(weights * values))\n",
    "        if isinstance(x, np.ndarray):\n",
    "            x = x.flatten()\n",
    "        res = sum(w_i * x_i for w_i, x_i in zip(self.weights, x)) + self.bias\n",
    "        output = res.sigmoid()\n",
    "        return output\n",
    "\n",
    "    def parameters(self) -> list[Value]:\n",
    "        return self.weights + [self.bias]\n",
    "\n",
    "    def param_count(self) -> int:\n",
    "        return len(self.weights + [self.bias])\n",
    "\n",
    "\n",
    "class Layer:\n",
    "    def __init__(self, num_inputs: int, num_outputs: int) -> None:\n",
    "        self.neurons = [Neuron(num_inputs) for _ in range(num_outputs)]\n",
    "\n",
    "    def __call__(self, x: np.ndarray) -> list[Value]:\n",
    "        outputs = [n(x) for n in self.neurons]\n",
    "        return outputs[0] if len(outputs) == 1 else outputs\n",
    "\n",
    "    def parameters(self) -> list:\n",
    "        params = [p for n in self.neurons for p in n.parameters()]\n",
    "        return params\n",
    "\n",
    "\n",
    "class MLP:\n",
    "    def __init__(self, num_inputs: int, num_outputs: list[int]) -> None:\n",
    "        self.size = [num_inputs] + num_outputs\n",
    "        self.layers = [\n",
    "            Layer(self.size[i], self.size[i + 1]) for i in range(len(num_outputs))\n",
    "        ]\n",
    "\n",
    "    def __call__(self, x: np.ndarray) -> Value:\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "    def parameters(self) -> list:\n",
    "        params = [p for l in self.layers for p in l.parameters()]\n",
    "        return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_img_path = \"../data/test_img.idx\"\n",
    "test_label_path = \"../data/test_label.idx\"\n",
    "fs, fl = parse_mnist_data(test_img_path, test_label_path, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_image(fs[np.random.choice(len(fs))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = Neuron(100)\n",
    "res = n(fs[5])\n",
    "res.backward()\n",
    "res\n",
    "# n.param_count()\n",
    "# lol = cross_entropy_loss(n(fs[5]), fl[5])\n",
    "# print(lol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# res.plot_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = Layer(100, 10)\n",
    "res2 = l(fs[5])\n",
    "res2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = MLP(100, [50, 10, 1])\n",
    "res3 = m(fs[5])\n",
    "res3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\frac{1}{N}\\Sigma_{i=1}^N(y_i\\cdot log(p_i) + (1-y_i)log(1-p_i))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def res_loss(y_pred: Value, y_gt) -> Value:\n",
    "    y_gt = Value(y_gt.item(), (), name=\"ground truth\")\n",
    "    loss = (y_gt - y_pred) ** 2\n",
    "    return loss\n",
    "\n",
    "\n",
    "def cross_entropy_loss(y_pred: Value, y_gt) -> Value:\n",
    "    y_gt = Value(y_gt.item(), ())\n",
    "    log_loss_positive = y_gt * y_pred.log()\n",
    "    log_loss_negative = (1 - y_gt) * (1 - y_pred).log()\n",
    "\n",
    "    loss = -(log_loss_positive + log_loss_negative)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trainingsloop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load images\n",
    "test_img_path = \"../data/test_img.idx\"\n",
    "test_label_path = \"../data/test_label.idx\"\n",
    "train_img, train_label = parse_mnist_data(test_img_path, test_label_path, 0, 1)\n",
    "\n",
    "# initialize MLP\n",
    "nin = 100\n",
    "nouts = [50, 10, 1]\n",
    "mlp = MLP(nin, nouts)\n",
    "parameters = mlp.parameters()\n",
    "\n",
    "len(parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter\n",
    "lr = 1e-3\n",
    "epochs = 50\n",
    "\n",
    "for e in range(epochs):\n",
    "    # forward pass\n",
    "    y_pred = [mlp(img) for img in train_img]\n",
    "    # backward pass\n",
    "    for p in parameters:\n",
    "        p.grad = 0.0\n",
    "    loss = sum(res_loss(yout, ygt) for yout, ygt in zip(y_pred, train_label.item()))\n",
    "    loss.backward()\n",
    "    print(e, loss.value)\n",
    "    # optimization\n",
    "    for p in parameters:\n",
    "        p.value -= lr * p.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_image(train_img[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp(train_img[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diy-mnist-nn-ZXZBkNkJ-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
