{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import graphviz\n",
    "from IPython.display import display\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Value:\n",
    "    def __init__(\n",
    "        self, value: float, ancestors: tuple[Value, ...] = (), name=\"\", operand=\"\"\n",
    "    ):\n",
    "        self.value = value\n",
    "        self.ancestors = ancestors\n",
    "        self.name = name\n",
    "        self.grad = 0.0\n",
    "        self._backward = lambda: None\n",
    "        self.operand = operand\n",
    "\n",
    "    # make values printable\n",
    "    def __repr__(self) -> str:\n",
    "        return f\"{self.name}, value={self.value}, grad={self.grad}\"\n",
    "\n",
    "    # Addition\n",
    "    def __add__(self, other: Value) -> Value:\n",
    "        if not isinstance(other, Value):\n",
    "            other = Value(other)\n",
    "        result = Value(self.value + other.value, (self, other), name=\"add\", operand=\"+\")\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += result.grad\n",
    "            other.grad += result.grad\n",
    "\n",
    "        result._backward = _backward\n",
    "        return result\n",
    "\n",
    "    def __iadd__(self, other: Value) -> Value:\n",
    "        if not isinstance(other, Value):\n",
    "            other = Value(other)\n",
    "        result = Value(\n",
    "            self.value + other.value, (self, other), name=\"iadd\", operand=\"+=\"\n",
    "        )\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += result.grad\n",
    "            other.grad += result.grad\n",
    "\n",
    "        result._backward = _backward\n",
    "        return result\n",
    "\n",
    "    def __radd__(self, other: Value):\n",
    "        return self + other\n",
    "\n",
    "    # Subtraktion\n",
    "    def __sub__(self, other: Value) -> Value:\n",
    "        if not isinstance(other, Value):\n",
    "            other = Value(other)\n",
    "        result = Value(self.value - other.value, (self, other), name=\"sub\", operand=\"-\")\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += 1.0 * result.grad\n",
    "            other.grad += -1.0 * result.grad\n",
    "\n",
    "        result._backward = _backward\n",
    "        return result\n",
    "\n",
    "    def __rsub__(self, other: Value) -> Value:\n",
    "        return self - other\n",
    "\n",
    "    # Multiplikation\n",
    "    def __mul__(self, other: Value) -> Value:\n",
    "        if not isinstance(other, Value):\n",
    "            other = Value(other)\n",
    "        result = Value(self.value * other.value, (self, other), name=\"mul\", operand=\"*\")\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += other.value * result.grad\n",
    "            other.grad += self.value * result.grad\n",
    "\n",
    "        result._backward = _backward\n",
    "        return result\n",
    "\n",
    "    def __rmul__(self, other: Value) -> Value:\n",
    "        return self * other\n",
    "\n",
    "    # Floatingpointdivision\n",
    "    def __truediv__(self, other: Value) -> Value:\n",
    "        if not isinstance(other, Value):\n",
    "            other = Value(other)\n",
    "        result = Value(self.value / other.value, (self, other), name=\"div\", operand=\"/\")\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += 1 / other.value * result.grad\n",
    "            other.grad += -self.value / other.value**2 * result.grad\n",
    "\n",
    "        result._backward = _backward\n",
    "        return result\n",
    "\n",
    "    def __rtruediv__(self, other: Value) -> Value:\n",
    "        return self / other\n",
    "\n",
    "    # Potenzierung (x**n)\n",
    "    def __pow__(self, other: Value) -> Value:\n",
    "        if not isinstance(other, Value):\n",
    "            other = Value(other)\n",
    "        result = Value(self.value**other.value, (self, other), name=\"pow\", operand=\"^\")\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += other.value * self.value ** (other.value - 1.0) * result.grad\n",
    "            # assert self.value >= 0, \"cannot compute log with negative base\n",
    "            other.grad += self.value**other.value * np.log(self.value) * result.grad\n",
    "            # print(self.grad, other.grad)\n",
    "\n",
    "        result._backward = _backward\n",
    "        return result\n",
    "\n",
    "    # Exponentierung (e**x)\n",
    "    def exp(self) -> Value:\n",
    "        result = Value(np.exp(self.value), (self,), name=\"exp\", operand=\"e^\")\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += result.value * result.grad\n",
    "\n",
    "        result._backward = _backward\n",
    "        return result\n",
    "\n",
    "    def log(self) -> Value:\n",
    "        result = Value(np.log(self.value), (self,), name=\"log\")\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += 1 / self.value * result.grad\n",
    "\n",
    "        result._backward = _backward\n",
    "        return result\n",
    "\n",
    "    # backwards up until this point\n",
    "    # Negation\n",
    "    def __neg__(self) -> Value:\n",
    "        result = Value(-self.value, (self,), name=\"neg\", operand=\"-\")\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += -result.grad\n",
    "\n",
    "        result._backward = _backward\n",
    "        return result\n",
    "\n",
    "    def sigmoid(self) -> Value:\n",
    "        sigmoid_value = 1 / (1 + np.exp(-self.value))\n",
    "        result = Value(sigmoid_value, (self,), name=\"sigmoid\")\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += sigmoid_value * (1 - sigmoid_value) * result.grad\n",
    "\n",
    "        result._backward = _backward\n",
    "        return result\n",
    "\n",
    "    # Vergleichsoperatoren <, >, >=, <=\n",
    "    def __lt__(self, other: Value) -> bool:\n",
    "        if not isinstance(other, Value):\n",
    "            other = Value(other)\n",
    "        return self.value < other.value\n",
    "\n",
    "    def __gt__(self, other: Value) -> bool:\n",
    "        if not isinstance(other, Value):\n",
    "            other = Value(other)\n",
    "        return self.value > other.value\n",
    "\n",
    "    def __le__(self, other: Value) -> bool:\n",
    "        if not isinstance(other, Value):\n",
    "            other = Value(other)\n",
    "        return self.value <= other.value\n",
    "\n",
    "    def __ge__(self, other: Value) -> bool:\n",
    "        if not isinstance(other, Value):\n",
    "            other = Value(other)\n",
    "        return self.value >= other.value\n",
    "\n",
    "    def backward(self) -> None:\n",
    "        # iterate through the graph, calculate gradients and update nodes\n",
    "        topo_sorted_nodes = []\n",
    "        visited = set()\n",
    "\n",
    "        # topological sort of the nodes\n",
    "        def build_topo(node: Value):\n",
    "            if node not in visited:\n",
    "                visited.add(node)\n",
    "                for ancestor in node.ancestors:\n",
    "                    build_topo(ancestor)\n",
    "                topo_sorted_nodes.append(node)\n",
    "\n",
    "        build_topo(self)\n",
    "\n",
    "        self.grad = 1.0\n",
    "        for node in reversed(topo_sorted_nodes):\n",
    "            node._backward()\n",
    "\n",
    "    def plot_graph(self):\n",
    "        # \"graph visualization python\", graphviz\n",
    "        dot = graphviz.Digraph(format=\"svg\", graph_attr={\"rankdir\": \"LR\"})\n",
    "\n",
    "        def add_nodes(dot: graphviz.Digraph, node: Value):\n",
    "            label = f\"{node.name}|value={node.value}|grad={node.grad}\"\n",
    "            unique_node_name = str(id(node))\n",
    "\n",
    "            # add value nodes to graph\n",
    "            dot.node(\n",
    "                name=unique_node_name,\n",
    "                label=label,\n",
    "                shape=\"record\",\n",
    "                color=(\n",
    "                    \"lightgreen\" if node.ancestors == () and node.name != \"\" else None\n",
    "                ),  # check if input\n",
    "                style=\"filled\",\n",
    "            )\n",
    "\n",
    "            if node.operand:  # check if there is an operand to display\n",
    "                op_name = unique_node_name + node.operand\n",
    "                # add operation node\n",
    "                dot.node(\n",
    "                    name=op_name,\n",
    "                    label=node.operand,\n",
    "                )\n",
    "                # draw edge from operand to result\n",
    "                dot.edge(op_name, unique_node_name)\n",
    "\n",
    "            # iterate through the ancestors to build the whole graph\n",
    "            for ancestor in node.ancestors:\n",
    "                ancestor_name = add_nodes(dot, ancestor)\n",
    "                if node.operand:\n",
    "                    # ensure ancestor edge goes to operand node if it exists\n",
    "                    dot.edge(ancestor_name, op_name)\n",
    "                else:\n",
    "                    dot.edge(ancestor_name, unique_node_name)\n",
    "\n",
    "            return unique_node_name\n",
    "\n",
    "        add_nodes(dot, self)\n",
    "        display(dot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_mnist_data(\n",
    "    idx_file_training_samples: str,\n",
    "    idx_file_training_labels: str,\n",
    "    number_1: int,\n",
    "    number_2: int,\n",
    ") -> tuple[np.ndarray, np.ndarray]:\n",
    "\n",
    "    training_labels = parse_mnist_labels(idx_file_training_labels)\n",
    "    training_samples = parse_mnist_images(idx_file_training_samples)\n",
    "\n",
    "    # filter only two numbers with a mask\n",
    "    mask = (training_labels.flatten() == number_1) | (\n",
    "        training_labels.flatten() == number_2\n",
    "    )\n",
    "    filtered_labels = training_labels[mask]\n",
    "    filtered_samples = training_samples[mask]\n",
    "\n",
    "    # downscale images with pillow\n",
    "    downscaled_samples = np.array(\n",
    "        # Image.Resampling.LANCZOS fÃ¼r Antialiasing\n",
    "        [\n",
    "            Image.fromarray(img).resize((10, 10), Image.Resampling.LANCZOS)\n",
    "            for img in filtered_samples\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    downscaled_samples = downscaled_samples / 255\n",
    "\n",
    "    return downscaled_samples, filtered_labels\n",
    "\n",
    "\n",
    "def parse_mnist_images(idx_file_path: str) -> np.ndarray:\n",
    "    with open(idx_file_path, \"rb\") as f:\n",
    "\n",
    "        # read magic number\n",
    "        f.read(4)\n",
    "        num_img = int.from_bytes(f.read(4), \"big\")\n",
    "        num_rows = int.from_bytes(f.read(4), \"big\")\n",
    "        num_cols = int.from_bytes(f.read(4), \"big\")\n",
    "\n",
    "        data = f.read()\n",
    "        out = np.ndarray((num_img, num_rows, num_cols), np.uint8, data)\n",
    "        return out\n",
    "\n",
    "\n",
    "def parse_mnist_labels(idx_file_path: str) -> np.ndarray:\n",
    "    with open(idx_file_path, \"rb\") as f:\n",
    "\n",
    "        # read magic number\n",
    "        f.read(4)\n",
    "        num_item = int.from_bytes(f.read(4), \"big\")\n",
    "\n",
    "        data = f.read()\n",
    "        out = np.ndarray((num_item, 1), np.uint8, data)\n",
    "        return out\n",
    "\n",
    "\n",
    "def plot_image(img: np.ndarray) -> plt.Figure:\n",
    "    assert len(img.shape) == 2, \"input must be 2-dimensional (single image)\"\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.axis(\"off\")\n",
    "    ax.imshow(img * 255, cmap=\"gray\")\n",
    "\n",
    "    plt.close()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neuron:\n",
    "    def __init__(self, num_inputs: int) -> None:\n",
    "        self.weights = [Value(np.random.random(size=None)) for _ in range(num_inputs)]\n",
    "        self.bias = Value(1.0, name=\"bias\")\n",
    "\n",
    "    def __call__(self, x: np.ndarray) -> Value:\n",
    "        # implement f(x) = activation (bias + sum(weights * values))\n",
    "        if isinstance(x, np.ndarray):\n",
    "            x = x.flatten()\n",
    "        res = sum(w_i * x_i for w_i, x_i in zip(self.weights, x)) + self.bias\n",
    "        output = res.sigmoid()\n",
    "        return output\n",
    "\n",
    "    def parameters(self) -> list[Value]:\n",
    "        return self.weights + [self.bias]\n",
    "\n",
    "    def param_count(self) -> int:\n",
    "        return len(self.weights + [self.bias])\n",
    "\n",
    "\n",
    "class Layer:\n",
    "    def __init__(self, num_inputs: int, num_outputs: int) -> None:\n",
    "        self.neurons = [Neuron(num_inputs) for _ in range(num_outputs)]\n",
    "\n",
    "    def __call__(self, x: np.ndarray) -> list[Value]:\n",
    "        outputs = [n(x) for n in self.neurons]\n",
    "        return outputs[0] if len(outputs) == 1 else outputs\n",
    "\n",
    "    def parameters(self) -> list:\n",
    "        params = [p for n in self.neurons for p in n.parameters()]\n",
    "        return params\n",
    "\n",
    "\n",
    "class MLP:\n",
    "    def __init__(self, num_inputs: int, num_outputs: list[int]) -> None:\n",
    "        self.size = [num_inputs] + num_outputs\n",
    "        self.layers = [\n",
    "            Layer(self.size[i], self.size[i + 1]) for i in range(len(num_outputs))\n",
    "        ]\n",
    "\n",
    "    def __call__(self, x: np.ndarray) -> Value:\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "    def parameters(self) -> list:\n",
    "        params = [p for l in self.layers for p in l.parameters()]\n",
    "        return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def res_loss(y_pred: Value, y_gt) -> Value:\n",
    "    y_gt = Value(y_gt.item(), (), name=\"ground truth\")\n",
    "    loss = (y_gt - y_pred) ** 2\n",
    "    return loss\n",
    "\n",
    "\n",
    "def cross_entropy_loss(y_pred: Value, y_gt) -> Value:\n",
    "    y_gt = Value(y_gt.item(), ())\n",
    "    log_loss_positive = y_gt * Value(np.log(y_pred))\n",
    "    log_loss_negative = (1 - y_gt) * Value(np.log(1 - y_pred))\n",
    "\n",
    "    loss = -(log_loss_positive + log_loss_negative)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load images\n",
    "test_img_path = \"../data/test_img.idx\"\n",
    "test_label_path = \"../data/test_label.idx\"\n",
    "train_img, train_label = parse_mnist_data(test_img_path, test_label_path, 0, 1)\n",
    "\n",
    "# initialize MLP\n",
    "nin = 100\n",
    "nouts = [50, 10, 1]\n",
    "mlp = MLP(nin, nouts)\n",
    "parameters = mlp.parameters()\n",
    "\n",
    "len(parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter\n",
    "lr = 1e-1\n",
    "epochs = 50\n",
    "batch = 100\n",
    "\n",
    "for e in range(epochs):\n",
    "    sample = np.random.shuffle(np.random.randint(0, train_img.shape[0], size=(batch)))\n",
    "    x = train_img[sample].reshape(-1, 10, 10)\n",
    "    y_gt = train_label[sample]\n",
    "    # forward pass\n",
    "    y_pred = [mlp(img) for img in x]\n",
    "    # backward pass\n",
    "    for p in parameters:\n",
    "        p.grad = 0.0\n",
    "    loss = sum(res_loss(ypred, ygt) for ypred, ygt in zip(y_pred, y_gt))\n",
    "    loss.backward()\n",
    "    print(f\"Epoche: {e}, Loss: {loss.value}\")\n",
    "    # optimization\n",
    "    for p in parameters:\n",
    "        p.value -= lr * p.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test should return 0\n",
    "print(mlp(train_img[1]).value)\n",
    "\n",
    "# Test 2 should return 1\n",
    "print(mlp(train_img[0]).value)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diy-mnist-nn-ZXZBkNkJ-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
