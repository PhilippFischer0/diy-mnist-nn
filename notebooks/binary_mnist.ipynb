{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import graphviz\n",
    "from IPython.display import display\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Literal\n",
    "import time\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Value:\n",
    "    def __init__(\n",
    "        self, value: float, ancestors: tuple[Value, ...] = (), name=\"\", operand=\"\"\n",
    "    ):\n",
    "        self.value = value\n",
    "        self.ancestors = ancestors\n",
    "        self.name = name\n",
    "        self.grad = 0.0\n",
    "        self._backward = lambda: None\n",
    "        self.operand = operand\n",
    "\n",
    "    # make values printable\n",
    "    def __repr__(self) -> str:\n",
    "        return f\"{self.name}, value={self.value}, grad={self.grad}\"\n",
    "\n",
    "    # Addition\n",
    "    def __add__(self, other: Value) -> Value:\n",
    "        if not isinstance(other, Value):\n",
    "            other = Value(other)\n",
    "        result = Value(self.value + other.value, (self, other), name=\"add\", operand=\"+\")\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += result.grad\n",
    "            other.grad += result.grad\n",
    "\n",
    "        result._backward = _backward\n",
    "        return result\n",
    "\n",
    "    def __iadd__(self, other: Value) -> Value:\n",
    "        if not isinstance(other, Value):\n",
    "            other = Value(other)\n",
    "        result = Value(\n",
    "            self.value + other.value, (self, other), name=\"iadd\", operand=\"+=\"\n",
    "        )\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += result.grad\n",
    "            other.grad += result.grad\n",
    "\n",
    "        result._backward = _backward\n",
    "        return result\n",
    "\n",
    "    def __radd__(self, other: Value):\n",
    "        return self + other\n",
    "\n",
    "    # Subtraktion\n",
    "    def __sub__(self, other: Value) -> Value:\n",
    "        if not isinstance(other, Value):\n",
    "            other = Value(other)\n",
    "        result = Value(self.value - other.value, (self, other), name=\"sub\", operand=\"-\")\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += 1.0 * result.grad\n",
    "            other.grad += -1.0 * result.grad\n",
    "\n",
    "        result._backward = _backward\n",
    "        return result\n",
    "\n",
    "    def __rsub__(self, other: Value) -> Value:\n",
    "        return self - other\n",
    "\n",
    "    # Multiplikation\n",
    "    def __mul__(self, other: Value) -> Value:\n",
    "        if not isinstance(other, Value):\n",
    "            other = Value(other)\n",
    "        result = Value(self.value * other.value, (self, other), name=\"mul\", operand=\"*\")\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += other.value * result.grad\n",
    "            other.grad += self.value * result.grad\n",
    "\n",
    "        result._backward = _backward\n",
    "        return result\n",
    "\n",
    "    def __rmul__(self, other: Value) -> Value:\n",
    "        return self * other\n",
    "\n",
    "    # Floatingpointdivision\n",
    "    def __truediv__(self, other: Value) -> Value:\n",
    "        if not isinstance(other, Value):\n",
    "            other = Value(other)\n",
    "        result = Value(self.value / other.value, (self, other), name=\"div\", operand=\"/\")\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += 1 / other.value * result.grad\n",
    "            other.grad += -self.value / other.value**2 * result.grad\n",
    "\n",
    "        result._backward = _backward\n",
    "        return result\n",
    "\n",
    "    def __rtruediv__(self, other: Value) -> Value:\n",
    "        return self / other\n",
    "\n",
    "    # Potenzierung (x**n)\n",
    "    def __pow__(self, other: Value) -> Value:\n",
    "        if not isinstance(other, Value):\n",
    "            other = Value(other)\n",
    "        result = Value(self.value**other.value, (self, other), name=\"pow\", operand=\"^\")\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += other.value * self.value ** (other.value - 1.0) * result.grad\n",
    "            # assert self.value >= 0, \"cannot compute log with negative base\n",
    "            other.grad += self.value**other.value * np.log(self.value) * result.grad\n",
    "            # print(self.grad, other.grad)\n",
    "\n",
    "        result._backward = _backward\n",
    "        return result\n",
    "\n",
    "    # Exponentierung (e**x)\n",
    "    def exp(self) -> Value:\n",
    "        result = Value(np.exp(self.value), (self,), name=\"exp\", operand=\"e^\")\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += result.value * result.grad\n",
    "\n",
    "        result._backward = _backward\n",
    "        return result\n",
    "\n",
    "    def log(self) -> Value:\n",
    "        result = Value(np.log(self.value), (self,), name=\"log\")\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += 1 / self.value * result.grad\n",
    "\n",
    "        result._backward = _backward\n",
    "        return result\n",
    "\n",
    "    # backwards up until this point\n",
    "    # Negation\n",
    "    def __neg__(self) -> Value:\n",
    "        result = Value(-self.value, (self,), name=\"neg\", operand=\"-\")\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += -result.grad\n",
    "\n",
    "        result._backward = _backward\n",
    "        return result\n",
    "\n",
    "    def sigmoid(self) -> Value:\n",
    "        sigmoid_value = 1 / (1 + np.exp(-self.value))\n",
    "        result = Value(sigmoid_value, (self,), name=\"sigmoid\")\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += sigmoid_value * (1 - sigmoid_value) * result.grad\n",
    "\n",
    "        result._backward = _backward\n",
    "        return result\n",
    "\n",
    "    # how to fix backward with ne values\n",
    "    def relu(self) -> Value:\n",
    "        result_value = self.value if self.value > 0 else 0.0\n",
    "        result = Value(result_value, (self,), name=\"ReLU\")\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += (self.value > 0) * result.grad\n",
    "\n",
    "        result._backward = _backward\n",
    "        return result\n",
    "\n",
    "    def leaky_relu(self, alpha=0.01) -> Value:\n",
    "        result_value = self.value if self.value > 0 else alpha * self.value\n",
    "        result = Value(result_value, (self,), name=\"LeakyReLU\")\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += (1 if self.value > 0 else alpha) * result.grad\n",
    "\n",
    "        result._backward = _backward\n",
    "        return result\n",
    "\n",
    "    # Vergleichsoperatoren <, >, >=, <=\n",
    "    def __lt__(self, other: Value) -> bool:\n",
    "        if not isinstance(other, Value):\n",
    "            other = Value(other)\n",
    "        return self.value < other.value\n",
    "\n",
    "    def __gt__(self, other: Value) -> bool:\n",
    "        if not isinstance(other, Value):\n",
    "            other = Value(other)\n",
    "        return self.value > other.value\n",
    "\n",
    "    def __le__(self, other: Value) -> bool:\n",
    "        if not isinstance(other, Value):\n",
    "            other = Value(other)\n",
    "        return self.value <= other.value\n",
    "\n",
    "    def __ge__(self, other: Value) -> bool:\n",
    "        if not isinstance(other, Value):\n",
    "            other = Value(other)\n",
    "        return self.value >= other.value\n",
    "\n",
    "    def backward(self) -> None:\n",
    "        # iterate through the graph, calculate gradients and update nodes\n",
    "        topo_sorted_nodes = []\n",
    "        visited = set()\n",
    "\n",
    "        # topological sort of the nodes\n",
    "        def build_topo(node: Value):\n",
    "            if node not in visited:\n",
    "                visited.add(node)\n",
    "                for ancestor in node.ancestors:\n",
    "                    build_topo(ancestor)\n",
    "                topo_sorted_nodes.append(node)\n",
    "\n",
    "        build_topo(self)\n",
    "\n",
    "        self.grad = 1.0\n",
    "        for node in reversed(topo_sorted_nodes):\n",
    "            node._backward()\n",
    "\n",
    "    def plot_graph(self):\n",
    "        # \"graph visualization python\", graphviz\n",
    "        dot = graphviz.Digraph(format=\"svg\", graph_attr={\"rankdir\": \"LR\"})\n",
    "\n",
    "        def add_nodes(dot: graphviz.Digraph, node: Value):\n",
    "            label = f\"{node.name}|value={node.value}|grad={node.grad}\"\n",
    "            unique_node_name = str(id(node))\n",
    "\n",
    "            # add value nodes to graph\n",
    "            dot.node(\n",
    "                name=unique_node_name,\n",
    "                label=label,\n",
    "                shape=\"record\",\n",
    "                color=(\n",
    "                    \"lightgreen\" if node.ancestors == () and node.name != \"\" else None\n",
    "                ),  # check if input\n",
    "                style=\"filled\",\n",
    "            )\n",
    "\n",
    "            if node.operand:  # check if there is an operand to display\n",
    "                op_name = unique_node_name + node.operand\n",
    "                # add operation node\n",
    "                dot.node(\n",
    "                    name=op_name,\n",
    "                    label=node.operand,\n",
    "                )\n",
    "                # draw edge from operand to result\n",
    "                dot.edge(op_name, unique_node_name)\n",
    "\n",
    "            # iterate through the ancestors to build the whole graph\n",
    "            for ancestor in node.ancestors:\n",
    "                ancestor_name = add_nodes(dot, ancestor)\n",
    "                if node.operand:\n",
    "                    # ensure ancestor edge goes to operand node if it exists\n",
    "                    dot.edge(ancestor_name, op_name)\n",
    "                else:\n",
    "                    dot.edge(ancestor_name, unique_node_name)\n",
    "\n",
    "            return unique_node_name\n",
    "\n",
    "        add_nodes(dot, self)\n",
    "        display(dot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_mnist_data(\n",
    "    idx_file_training_samples: str,\n",
    "    idx_file_training_labels: str,\n",
    "    number_1: int,\n",
    "    number_2: int,\n",
    ") -> tuple[np.ndarray, np.ndarray]:\n",
    "\n",
    "    training_labels = parse_mnist_labels(idx_file_training_labels)\n",
    "    training_samples = parse_mnist_images(idx_file_training_samples)\n",
    "\n",
    "    # filter only two numbers with a mask\n",
    "    mask = (training_labels.flatten() == number_1) | (\n",
    "        training_labels.flatten() == number_2\n",
    "    )\n",
    "    filtered_labels = training_labels[mask]\n",
    "    filtered_samples = training_samples[mask]\n",
    "\n",
    "    # TODO: Image.resize() Algorithmus -> Recherche\n",
    "    # -LANCZOS als Algorithmus zur Bildverkleinerung -> sinc(x) = sin(πx) / (πx)\n",
    "    # -gemacht zum downscalen von Bildern ->\n",
    "    # downscale images with pillow\n",
    "    downscaled_samples = np.array(\n",
    "        # Image.Resampling.LANCZOS\n",
    "        [\n",
    "            Image.fromarray(img).resize((10, 10), Image.Resampling.LANCZOS)\n",
    "            for img in filtered_samples\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    downscaled_samples = downscaled_samples / 255\n",
    "\n",
    "    return downscaled_samples, filtered_labels\n",
    "\n",
    "\n",
    "def parse_mnist_images(idx_file_path: str) -> np.ndarray:\n",
    "    with open(idx_file_path, \"rb\") as f:\n",
    "\n",
    "        # read magic number\n",
    "        f.read(4)\n",
    "        num_img = int.from_bytes(f.read(4), \"big\")\n",
    "        num_rows = int.from_bytes(f.read(4), \"big\")\n",
    "        num_cols = int.from_bytes(f.read(4), \"big\")\n",
    "\n",
    "        data = f.read()\n",
    "        out = np.ndarray((num_img, num_rows, num_cols), np.uint8, data)\n",
    "        return out\n",
    "\n",
    "\n",
    "def parse_mnist_labels(idx_file_path: str) -> np.ndarray:\n",
    "    with open(idx_file_path, \"rb\") as f:\n",
    "\n",
    "        # read magic number\n",
    "        f.read(4)\n",
    "        num_item = int.from_bytes(f.read(4), \"big\")\n",
    "\n",
    "        data = f.read()\n",
    "        out = np.ndarray((num_item, 1), np.uint8, data)\n",
    "        return out\n",
    "\n",
    "\n",
    "def plot_image(img: np.ndarray) -> plt.Figure:\n",
    "    assert len(img.shape) == 2, \"input must be 2-dimensional (single image)\"\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.axis(\"off\")\n",
    "    ax.imshow(img * 255, cmap=\"gray\")\n",
    "\n",
    "    plt.close()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0xDEADBEEF)\n",
    "\n",
    "\n",
    "class Neuron:\n",
    "    def __init__(self, num_inputs: int) -> None:\n",
    "        self.weights = [Value(np.random.random(size=None)) for _ in range(num_inputs)]\n",
    "        self.bias = Value(0.0, name=\"bias\")\n",
    "\n",
    "    def __call__(self, x: np.ndarray) -> Value:\n",
    "        # implement f(x) = activation (bias + sum(weights * values))\n",
    "        if isinstance(x, np.ndarray):\n",
    "            x = x.flatten()\n",
    "        res = sum(w_i * x_i for w_i, x_i in zip(self.weights, x)) + self.bias\n",
    "        return res\n",
    "\n",
    "    def parameters(self) -> list[Value]:\n",
    "        return self.weights + [self.bias]\n",
    "\n",
    "    def param_count(self) -> int:\n",
    "        return len(self.weights + [self.bias])\n",
    "\n",
    "\n",
    "class Layer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_inputs: int,\n",
    "        num_outputs: int,\n",
    "        use_activation: Literal[\"relu\", \"sigmoid\"],\n",
    "    ) -> None:\n",
    "        self.neurons = [Neuron(num_inputs) for _ in range(num_outputs)]\n",
    "        self.use_activation = use_activation\n",
    "\n",
    "    def __call__(self, x: np.ndarray) -> list[Value]:\n",
    "        outputs = [n(x) for n in self.neurons]\n",
    "        if self.use_activation == \"relu\":\n",
    "            return [o.relu() for o in outputs]\n",
    "        return [o.sigmoid() for o in outputs]\n",
    "\n",
    "    def parameters(self) -> list:\n",
    "        params = [p for n in self.neurons for p in n.parameters()]\n",
    "        return params\n",
    "\n",
    "\n",
    "class MLP:\n",
    "    def __init__(self, num_inputs: int, num_hidden: list[int], num_out: int) -> None:\n",
    "        size = [num_inputs] + num_hidden\n",
    "        self.layers = [\n",
    "            Layer(size[i], size[i + 1], \"relu\") for i in range(len(num_hidden))\n",
    "        ] + [Layer(num_hidden[-1], num_out, \"sigmoid\")]\n",
    "\n",
    "    def __call__(self, x: np.ndarray) -> Value:\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x[0]\n",
    "\n",
    "    def parameters(self) -> list:\n",
    "        params = [p for l in self.layers for p in l.parameters()]\n",
    "        return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = Value(2.9)\n",
    "b = Value(3.5)\n",
    "c = Value(-1.0)\n",
    "\n",
    "res = a * b / c\n",
    "\n",
    "foo = res.sigmoid()\n",
    "foo.backward()\n",
    "\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = Value(2.9)\n",
    "b = Value(3.5)\n",
    "c = Value(-1.0)\n",
    "\n",
    "res = a * b * c\n",
    "\n",
    "foo = res.relu()\n",
    "foo.backward()\n",
    "\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def res_loss(y_pred: Value, y_gt) -> Value:\n",
    "    y_gt = Value(y_gt.item(), (), name=\"ground truth\")\n",
    "    loss = (y_gt - y_pred) ** 2\n",
    "    return loss\n",
    "\n",
    "\n",
    "def cross_entropy_loss(y_pred: Value, y_gt) -> Value:\n",
    "    epsilon = 1e-15\n",
    "    y_pred_value_clipped = np.clip(y_pred.value, epsilon, 1 - epsilon)\n",
    "\n",
    "    y_gt = Value(y_gt.item(), ())\n",
    "    log_loss_positive = y_gt.value * np.log(y_pred_value_clipped)\n",
    "    log_loss_negative = (1 - y_gt.value) * np.log(1 - y_pred_value_clipped)\n",
    "\n",
    "    loss = Value(\n",
    "        -(log_loss_positive + log_loss_negative),\n",
    "        (y_pred, y_gt),\n",
    "        name=\"cross_entropy_loss\",\n",
    "    )\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load images\n",
    "train_img_path = \"../data/train_img.idx\"\n",
    "train_label_path = \"../data/train_label.idx\"\n",
    "test_img_path = \"../data/test_img.idx\"\n",
    "test_label_path = \"../data/test_label.idx\"\n",
    "\n",
    "train_img, train_label = parse_mnist_data(train_img_path, train_label_path, 0, 1)\n",
    "# only get the first 1000 images and labels\n",
    "train_img = train_img[:1000]\n",
    "train_label = train_label[:1000]\n",
    "\n",
    "test_img, test_label = parse_mnist_data(test_img_path, test_label_path, 0, 1)\n",
    "\n",
    "# initialize MLP\n",
    "nin = 100\n",
    "n_hidden = [10]\n",
    "nout = 1\n",
    "mlp = MLP(nin, n_hidden, nout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0xDEADBEEF)\n",
    "# Hyperparameter\n",
    "lr = 1e-3\n",
    "epochs = 1\n",
    "batch_size = 30\n",
    "num_img = train_img.shape[0]\n",
    "num_batches = int(num_img / batch_size) + 1\n",
    "\n",
    "losses_train = []\n",
    "correct_test_pred = 0\n",
    "correct_train_pred = 0\n",
    "accuracies_test = []\n",
    "accuracies_train = []\n",
    "times = []\n",
    "\n",
    "test = mlp.parameters()\n",
    "for e in range(epochs):\n",
    "    # Genauigkeit berechnen 1 pro Epoche + Plot für Training und Test\n",
    "    print(\"calculating accuracies...\")\n",
    "    for img_test, lab_test, img_train, lab_train in zip(\n",
    "        test_img, test_label, train_img, train_label\n",
    "    ):\n",
    "        pred_train = mlp(img_test)\n",
    "        pred_test = mlp(img_train)\n",
    "        if np.fabs(pred_test.value - lab_test.item()) < 0.5:\n",
    "            correct_test_pred += 1\n",
    "        if np.fabs(pred_train.value - lab_train.item()) < 0.5:\n",
    "            correct_train_pred += 1\n",
    "\n",
    "    accuracies_test.append(correct_test_pred / len(test_img))\n",
    "    accuracies_train.append(correct_train_pred / len(train_img))\n",
    "    print(\"done\")\n",
    "\n",
    "    # Epochendauer ausgeben\n",
    "    start_time = time.process_time()\n",
    "\n",
    "    idx = np.random.permutation(np.arange(num_img))\n",
    "    # inplace for better cache usage\n",
    "    train_img[:] = train_img[idx]\n",
    "    train_label[:] = train_label[idx]\n",
    "\n",
    "    for b in range(num_batches):\n",
    "        start_sample = b * batch_size\n",
    "        end_sample = min((b + 1) * batch_size, num_img)\n",
    "        x = train_img[start_sample:end_sample].reshape(-1, 10, 10)\n",
    "        y_gt = train_label[start_sample:end_sample]\n",
    "\n",
    "        # zero grad\n",
    "        for p in mlp.parameters():\n",
    "            p.grad = 0.0\n",
    "\n",
    "        # forward pass\n",
    "        y_pred = [mlp(img) for img in x]\n",
    "\n",
    "        # backward pass\n",
    "        outputs = [cross_entropy_loss(ypred, ygt) for ypred, ygt in zip(y_pred, y_gt)]\n",
    "        loss = sum(outputs) / len(outputs)\n",
    "        loss.backward()\n",
    "        (\n",
    "            print(f\"Epoche: {e+1}, Batch: {b+1} / {num_batches} Loss: {loss.value}\")\n",
    "            if b % 5 == 0 or b + 1 == num_batches\n",
    "            else None\n",
    "        )\n",
    "        losses_train.append(loss.value)\n",
    "\n",
    "        # optimization\n",
    "        for p in mlp.parameters():\n",
    "            # print(p.grad)\n",
    "            p.value -= lr * p.grad\n",
    "\n",
    "    correct_train_pred = 0\n",
    "    correct_test_pred = 0\n",
    "    end_time = time.process_time()\n",
    "    times.append(end_time - start_time)\n",
    "    print(f\"Epoche {e+1}: {times[e]} s\")\n",
    "\n",
    "assert mlp.parameters() != test, \"Parameter werden nicht geändert!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss von Testdaten\n",
    "epochs = 10\n",
    "batch_size = 30\n",
    "num_img = test_img.shape[0]\n",
    "num_batches = int(num_img / batch_size) + 1\n",
    "\n",
    "losses_test = []\n",
    "\n",
    "for e in range(epochs):\n",
    "    print(f\"starting epoch {e+1}...\")\n",
    "    for b in range(num_batches):\n",
    "        start_sample = b * batch_size\n",
    "        end_sample = min((b + 1) * batch_size, num_img)\n",
    "        x_test = test_img[start_sample:end_sample]\n",
    "        y_gt_test = test_label[start_sample:end_sample]\n",
    "\n",
    "        # forward pass\n",
    "        y_pred_test = [mlp(img) for img in x_test]\n",
    "\n",
    "        # calculate loss\n",
    "        outputs_test = [\n",
    "            cross_entropy_loss(ypred, ygt) for ypred, ygt in zip(y_pred_test, y_gt_test)\n",
    "        ]\n",
    "\n",
    "        loss_test = sum(outputs_test) / len(outputs_test)\n",
    "        losses_test.append(loss_test.value)\n",
    "\n",
    "    print(f\"epoch {e+1} done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.line(x=range(len(times)), y=times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.line(x=range(len(losses_train)), y=losses_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss von Test-Datensatz\n",
    "px.line(x=range(len(losses_test)), y=losses_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.line(x=range(len(accuracies_test)), y=accuracies_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.line(x=range(len(accuracies_train)), y=accuracies_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diy-mnist-nn-ZXZBkNkJ-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
